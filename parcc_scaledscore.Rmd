---
title: "PARCC Scaled Scores"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
---

*This is Part 4 in my series on 'Understanding PARCC'. [Part 1](parcc.html) covered measuring ability and [Part 2](parcc_test.html) provided an overview of the PARCC test. [Part 3](parcc_irt.html) is an introduciton to Item Response Theory.*


#Setting Cut Scores

To transform the ability estimates from PARCC's Item Response Theory model into scaled scores, PARCC essentially had a bunch of experts take the test pretending to be students at different performance levels. 

PARCC came up with written descriptions of what characteristics a student would have if they were at each of the 5 performance levels. They then asked a panel of experts to review a bunch of items and say, for each item, how many points they thought a student at each level would score if they were to take the item, based on the written description of students at that level. 

PARCC used the information from this process as if it were student responses to the actual items, and calculated IRT ability estimates for these theoretical students. 

PARCC chose 700 as the cut score for Level 2 and 750 as the cut score for level 4. That is, the consensus raw scores from the professors-pretending-to-be-students process were given a 700 for the level 2 imaginary students and a 750 for the level 4 imaginary students. They then used these anchors to proportionally transform the IRT results into scaled scores. 

PARCC then arbitrarily set the Level 3 cut score to 725, and set the Level 5 cut score by grade level based on the distribution of IRT results. This means that the 725 Level 3 cut score was somewhat arbitrary, and PARCC recommends caution in using the Level 3 cut score because there may not have been actual students at the level of IRT performance that translates to 725. 

PARCC also set a cut score for the maximum and minimum scores on the test. I am having difficulty finding information on how these were set, but the take away is that earning a 650 does not mean that you got zero questions correct and earning an 850 does not mean that you got 100% of questions correct -- the raw score cutoff for an 850 is less than 100% of items correct. 

PARCC went through separate but related scaling processes for creating the subclaim scores. 

PARCC's technical report shows an example raw score to scaled score conversion table (excerpt [here](parcc_rawscaled.html)).

#Scaled Score Distributions

PARCC's technical report has helpful charts showing the distribution of scaled scores (excerpt [here](parcc_scaled_dist.html)). There are a couple of things to note about these charts. 

The distributions of scaled scores are approximately normal, which means that a student’s percentile has a nonlinear relationship to their scaled score. 

Looking at the charts, many of the distributions look tight and shifted to the left-- the charts are tall, skinny, and to the left of the middle. This means that relatively few students scored very high on PARCC. 

The distributions are quite different across grade and content levels. For example, in [third grade ELA](parcc_scaled_dist.html#ela) the curve is relatively fat. Many students scored high and many students scored low. 

In [sixth grade ELA](parcc_scaled_dist.html#ela), the curve is might tighter. Students were clustered around the average score. The top 20th percentile of students would have a lower score in 6th grade ELA than in 3rd grade ELA, and the bottom 20% of students would have a higher score in 6th grade ELA than in 3rd grade ELA. 

The scaled [score distributions for writing](parcc_scaled_dist.html#writing) are less normal. This is likely for two reasons: there were fewer writing items, and the test was normed too high to get information about all students. The bump on the left hand side is characteristic of tests where many students score zero or close to zero. 

#Interpreting scaled scores

PARCC scaled scores are an ordinal measure, which means they provide information about relative performance. A score of 750 is relatively higher than a score of 740. How much higher? We can’t say. 

We do know that 750 is the cut score for Level 4 and 700 is the cut score for Level 2, and that these cut score was set based on how a panel of experts thought that Level 2 and 4 students would perform. So we can say that 740 is relatively lower than the expert consensus for Level 4 performance and relatively higher than the expert consensus for Level 2 performance. 

An example: based on the distributions of scaled scores, let’s say that 1% of students scored 850 and 2% of students scored 840 or above. Therefore a 10 point gain in your scaled score from 840 to 850 would move you from the top 2% of students to the top 1% of students. 

Let’s also say that 50% of students scored 735 or higher and 40% of students scored 740 or higher. In this case, a 5 point improvement would increase your percentile by 10 percentage points. 

Which is more meaningful? An improvement of 10 scaled score points or an improvement of 10 percentile points? I don’t know. They are both ordinal scales. 

Relative differences in scores depend on the difficulty of the material assessed. Let’s say a test assessed three skills, reciting the alphabet, recognizing letters, and reading fluently. The test is scaled from 0 to 3, where 0 is no literacy and 3 is reading fluently. While they are the same in our scale, the true difference between a 1 and a 2 (reciting the alphabet versus recognizing letters) is smaller than the difference between a 2 and 3 (recognizing letters versus reading fluently) because it is much hard to go from recognizing letter to reading fluently.^[Example [source](https://www.aeaweb.org/articles?id=10.1257/jep.30.3.85)]

But what if we think that recognizing letters is a much more critical skill for success in life, and we weigh that skill much higher? Then the gap could be reversed, and we could weigh an improvement from 1 to 2 more than an improvement from 2 to 3.

##Take-aways

It would be overly-constraining to only make ordinal claims about student performance -- we should not become paralyzed by the technical details. Some guidance could be:

1. Large differences are probably more meaningful than small differences, even if we can’t say precisely how much more.

2. We might be especially worried about particularly low scores. Very low scoring students could be even further behind than they seem if they are missing foundational material -- it is likely a nonlinear difference.

3. We might be particularly proud of very high scoring students. If they mastered all of the material, that is at least impressive, if not every impressive in the case of nonlinearities in difficulty of content.

<a href="parcc_ma.html"><button type="button" class="btn btn-primary">Continue to Part 5: Massachusetts Framework</button></a>